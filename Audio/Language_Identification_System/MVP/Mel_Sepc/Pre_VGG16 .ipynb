{"cells":[{"cell_type":"code","execution_count":85,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2793,"status":"ok","timestamp":1666867606347,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"-62PgMRMa2Qt","outputId":"4a34be2b-cf61-442a-b0a3-2635b3c1e6f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.1)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"]}],"source":["! pip install torchmetrics"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666867606348,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"4gz4ppuNINtr"},"outputs":[],"source":["# PyTorch\n","from torchvision import transforms, datasets, models\n","import torch\n","from torch import optim, cuda\n","from torch.utils.data import DataLoader, sampler\n","import torch.nn as nn\n","from torchmetrics import ConfusionMatrix\n","import warnings\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","\n","# Data science tools\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","# Image manipulations\n","from PIL import Image\n","# Useful for examining network\n","from torchsummary import summary\n","# Timing utility\n","from timeit import default_timer as timer\n","\n","# Visualizations\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","train_on_gpu = cuda.is_available()\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666867606348,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"WH1OGpmipvMn"},"outputs":[],"source":["PathToDF = \"/content/drive/MyDrive/Models1.csv\"\n","DataSetName= \"Mel128_200_M_12088\"\n","RandomState= 42"]},{"cell_type":"markdown","metadata":{"id":"iUzsbuksqI8f"},"source":["# Func"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1666867606679,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"Ehj6lNe2ptQX"},"outputs":[],"source":["def Save_To_CSV(PathToDF, df):\n","        if os.path.exists(PathToDF):\n","            df.to_csv(PathToDF, mode='a', header=False)\n","        else :\n","            df.to_csv(PathToDF)"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666867606679,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"eSOpMNJtqIOA"},"outputs":[],"source":["def test(model, test_loader, criterion, CM=False):\n","  with torch.no_grad():\n","    model.eval()\n","    test_acc = 0\n","    preidcted = np.array([])\n","    ALL_Labels= np.array([])\n","\n","    confusionMatrix= np.zeros((7,7))\n","    for images, labels in test_loader:\n","\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      \n","      _, pred = torch.max(outputs, dim=1)\n","      correct_tensor = pred.eq(labels.data.view_as(pred))\n","      accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","      test_acc += accuracy.item() * images.size(0)\n","      if CM:\n","        preidcted =np.append(preidcted ,pred.to(\"cpu\").numpy())\n","        ALL_Labels =np.append(ALL_Labels ,labels.to(\"cpu\").numpy())\n","    \n","\n","    test_acc = test_acc / len(test_loader.dataset)\n","\n","    if CM ==True:\n","      confmat = ConfusionMatrix(num_classes=7)\n","      confusionMatrix= np.array(confmat(torch.tensor(preidcted.astype(\"int32\")), torch.tensor(ALL_Labels.astype(\"int32\"))))\n","\n","    return test_acc, confusionMatrix"]},{"cell_type":"markdown","metadata":{"id":"vjY_D4pQKQRO"},"source":["# Load Data"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666867606680,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"uayLZgW4O38l"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666867606680,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"OQ7ZvzApt8dY","outputId":"a399e394-02d2-478d-bb05-37514297d7b0"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f74de6477f0>"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(RandomState)"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1944,"status":"ok","timestamp":1666867608620,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"m7IAkDyQKPaW","outputId":"db3a22c1-3d0c-4e5d-cc9b-0ec92f824e4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666867608621,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"GV3LOc-CKT_2"},"outputs":[],"source":["import warnings\n","from IPython.display import clear_output\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1666867608953,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"sv3VD6JwKURW"},"outputs":[],"source":["transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n","\n","dataset = datasets.ImageFolder(f'/content/drive/MyDrive/Data/{DataSetName}', transform=transform)\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n","\n","\n","train_data = int(len(dataset)*0.8)\n","valid_data= int((len(dataset) - train_data)/2)\n","test_data = int(len(dataset) - train_data - valid_data)\n","\n","train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_data, valid_data, test_data])\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SI2PMn3yKWfe"},"source":["# Train pre trained VGG16"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":1332,"status":"ok","timestamp":1666867610283,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"ErBZBQaGJe-H"},"outputs":[],"source":["import torchvision.models as models\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = models.vgg16(pretrained=True)"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666867610283,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"LtPhQmvRJh2O","outputId":"e53221f9-c304-42cc-de56-178bd1cb288b"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1666867610284,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"BhXXQCXCJp4u"},"outputs":[],"source":["def get_pretrained_model(model_name):\n","\n","    if model_name == 'vgg16':\n","        model = models.vgg16(pretrained=True)\n","\n","        # Freeze early layers\n","        # for param in model.parameters():\n","        #     param.requires_grad = False\n","        n_inputs = model.classifier[6].in_features\n","\n","        # Add on classifier\n","        model.classifier[6] = nn.Sequential(\n","            nn.Linear(n_inputs, 4096), nn.ReLU(), nn.Dropout(0.2),\n","            nn.Linear(4096, 7), nn.LogSoftmax(dim=1))\n","\n","    # Move to gpu and parallelize\n","    if train_on_gpu:\n","        model = model.to('cuda')\n","\n","    return model"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":1837,"status":"ok","timestamp":1666867612117,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"5rPoi6ZKJz09"},"outputs":[],"source":["model = get_pretrained_model('vgg16')"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666867612117,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"UDuy1PHhJ3KF"},"outputs":[],"source":["# summary(model, input_size=(3, 224, 224), batch_size=32, device=\"cuda\")"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666867612118,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"TityKKUqJ9Ud","outputId":"9dfb7c44-d128-4e6a-caa3-1ba94b64082f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequential(\n","  (0): Linear(in_features=4096, out_features=4096, bias=True)\n","  (1): ReLU()\n","  (2): Dropout(p=0.2, inplace=False)\n","  (3): Linear(in_features=4096, out_features=7, bias=True)\n","  (4): LogSoftmax(dim=1)\n",")\n"]}],"source":["print(model.classifier[6])\n"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1666867612118,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"e9QxRPxGKIFV","outputId":"45c98c55-cadd-4563-bb57-3329d31a526d"},"outputs":[{"data":{"text/plain":["[(0, 'ar'), (1, 'de'), (2, 'en'), (3, 'es'), (4, 'fr'), (5, 'it'), (6, 'pt')]"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["model.class_to_idx = dataset.class_to_idx\n","model.idx_to_class = {\n","    idx: class_\n","    for class_, idx in model.class_to_idx.items()\n","}\n","list(model.idx_to_class.items())[:7]"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666867612118,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"7-OWJg-lKLV9"},"outputs":[],"source":["def train(model,criterion,optimizer,train_loader,valid_loader,save_file_name,\n","          max_epochs_stop=3,n_epochs=20,print_every=1):\n","    \"\"\"Train a PyTorch Model\n","\n","    Params\n","    --------\n","        model (PyTorch model): cnn to train\n","        criterion (PyTorch loss): objective to minimize\n","        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n","        train_loader (PyTorch dataloader): training dataloader to iterate through\n","        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n","        save_file_name (str ending in '.pt'): file path to save the model state dict\n","        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n","        n_epochs (int): maximum number of training epochs\n","        print_every (int): frequency of epochs to print training stats\n","\n","    Returns\n","    --------\n","        model (PyTorch model): trained cnn with best weights\n","        history (DataFrame): history of train and validation loss and accuracy\n","    \"\"\"\n","\n","    # Early stopping intialization\n","    epochs_no_improve = 0\n","    valid_loss_min = np.Inf\n","    valid_acc_max = 0\n","\n","    valid_max_acc = 0\n","    history = []\n","\n","    # Number of epochs already trained (if using loaded in model weights)\n","    try:\n","        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n","    except:\n","        model.epochs = 0\n","        print(f'Starting Training from Scratch.\\n')\n","\n","    overall_start = timer()\n","\n","    # Main loop\n","    for epoch in range(n_epochs):\n","\n","        # keep track of training and validation loss each epoch\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        train_acc = 0\n","        valid_acc = 0\n","\n","        # Set to training\n","        model.train()\n","        start = timer()\n","\n","        # Training loop\n","        for ii, (data, target) in enumerate(train_loader):\n","            # Tensors to gpu\n","            if train_on_gpu:\n","                data, target = data.cuda(), target.cuda()\n","\n","            # Clear gradients\n","            optimizer.zero_grad()\n","            # Predicted outputs are log probabilities\n","            output = model(data)\n","\n","            # Loss and backpropagation of gradients\n","            loss = criterion(output, target)\n","            loss.backward()\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            # Track train loss by multiplying average loss by number of examples in batch\n","            train_loss += loss.item() * data.size(0)\n","\n","            # Calculate accuracy by finding max log probability\n","            _, pred = torch.max(output, dim=1)\n","            correct_tensor = pred.eq(target.data.view_as(pred))\n","            # Need to convert correct tensor from int to float to average\n","            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","            # Multiply average accuracy times the number of examples in batch\n","            train_acc += accuracy.item() * data.size(0)\n","\n","            # Track training progress\n","            print(\n","                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n","                end='\\r')\n","\n","        # After training loops ends, start validation\n","        else:\n","            model.epochs += 1\n","\n","            # Don't need to keep track of gradients\n","            with torch.no_grad():\n","                # Set to evaluation mode\n","                model.eval()\n","\n","                # Validation loop\n","                for data, target in valid_loader:\n","                    # Tensors to gpu\n","                    if train_on_gpu:\n","                        data, target = data.cuda(), target.cuda()\n","\n","                    # Forward pass\n","                    output = model(data)\n","\n","                    # Validation loss\n","                    loss = criterion(output, target)\n","                    # Multiply average loss times the number of examples in batch\n","                    valid_loss += loss.item() * data.size(0)\n","\n","                    # Calculate validation accuracy\n","                    _, pred = torch.max(output, dim=1)\n","                    correct_tensor = pred.eq(target.data.view_as(pred))\n","                    accuracy = torch.mean(\n","                        correct_tensor.type(torch.FloatTensor))\n","                    # Multiply average accuracy times the number of examples\n","                    valid_acc += accuracy.item() * data.size(0)\n","\n","                # Calculate average losses\n","                train_loss = train_loss / len(train_loader.dataset)\n","                valid_loss = valid_loss / len(valid_loader.dataset)\n","\n","                # Calculate average accuracy\n","                train_acc = train_acc / len(train_loader.dataset)\n","                valid_acc = valid_acc / len(valid_loader.dataset)\n","\n","                history.append([train_loss, valid_loss, train_acc, valid_acc])\n","\n","                # Print training and validation results\n","                if (epoch + 1) % print_every == 0:\n","                    print(\n","                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n","                    )\n","                    print(\n","                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n","                    )\n","\n","                # Save the model if validation loss decreases\n","                if valid_loss < valid_loss_min:\n","                # if valid_acc > valid_acc_max:\n","                    # Save model\n","                    torch.save(model.state_dict(), save_file_name)\n","                    # Track improvement\n","                    epochs_no_improve = 0\n","                    valid_loss_min = valid_loss\n","                    valid_acc_max= valid_acc\n","                    # valid_best_acc = valid_acc\n","                    best_epoch = epoch\n","\n","                # Otherwise increment count of epochs with no improvement\n","                else:\n","                    epochs_no_improve += 1\n","                    # Trigger early stopping\n","                    if epochs_no_improve >= max_epochs_stop:\n","                        print(\n","                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc_max:.2f}%'\n","                        )\n","                        total_time = timer() - overall_start\n","                        print(\n","                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n","                        )\n","\n","                        # Load the best state dict\n","                        model.load_state_dict(torch.load(save_file_name))\n","                        # Attach the optimizer\n","                        model.optimizer = optimizer\n","\n","                        # Format history\n","                        history = pd.DataFrame(\n","                            history,\n","                            columns=[\n","                                'train_loss', 'valid_loss', 'train_acc',\n","                                'valid_acc'\n","                            ])\n","                        return model, history\n","\n","    # Attach the optimizer\n","    model.optimizer = optimizer\n","    # Record overall time and print out stats\n","    total_time = timer() - overall_start\n","    print(\n","        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n","    )\n","    print(\n","        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n","    )\n","    # Format history\n","    history = pd.DataFrame(\n","        history,\n","        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n","    return model, history"]},{"cell_type":"markdown","metadata":{"id":"embCEpGnKozO"},"source":["## optim and loss"]},{"cell_type":"code","execution_count":102,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1666867612118,"user":{"displayName":"abdallah Ragab_3","userId":"02351025938243732401"},"user_tz":-120},"id":"n4fK1SI9KOa-"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay = 0.005, momentum = 0.9)  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YyeKI2kKv9U","outputId":"c9d9cf87-8b97-4b13-94aa-5a94fb216539"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Training from Scratch.\n","\n","\n","Epoch: 0 \tTraining Loss: 1.3622 \tValidation Loss: 1.0124\n","\t\tTraining Accuracy: 44.89%\t Validation Accuracy: 58.97%\n","\n","Epoch: 1 \tTraining Loss: 0.9020 \tValidation Loss: 0.7689\n","\t\tTraining Accuracy: 64.95%\t Validation Accuracy: 71.05%\n","\n","Epoch: 2 \tTraining Loss: 0.7669 \tValidation Loss: 0.6093\n","\t\tTraining Accuracy: 71.17%\t Validation Accuracy: 77.50%\n","\n","Epoch: 3 \tTraining Loss: 0.6425 \tValidation Loss: 0.6987\n","\t\tTraining Accuracy: 75.81%\t Validation Accuracy: 73.78%\n","\n","Epoch: 4 \tTraining Loss: 0.5699 \tValidation Loss: 0.6572\n","\t\tTraining Accuracy: 78.66%\t Validation Accuracy: 76.10%\n","\n","Epoch: 5 \tTraining Loss: 0.4902 \tValidation Loss: 0.8427\n","\t\tTraining Accuracy: 82.19%\t Validation Accuracy: 72.87%\n","\n","Epoch: 6 \tTraining Loss: 0.4598 \tValidation Loss: 0.4313\n","\t\tTraining Accuracy: 82.89%\t Validation Accuracy: 84.53%\n"]}],"source":["rd= np.random.randint(100000)\n","\n","modelName= f\"VGG_{DataSetName}_{rd}\"\n","\n","model, history = train(\n","    model,\n","    criterion,\n","    optimizer,\n","    train_loader,\n","    valid_loader,\n","    save_file_name=f\"/content/drive/MyDrive/Pre_VGG_models/{modelName}.pt\",\n","    max_epochs_stop=6,\n","    n_epochs=100,\n","    print_every=1)"]},{"cell_type":"markdown","metadata":{"id":"Yd4WMqfBpeJ3"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jq8JTpyJpdff"},"outputs":[],"source":["test_acc, CM_Test  = test(model.cuda(), test_loader, criterion, True)\n","print(f'The model has achieved an accuracy of {100 * test_acc:.2f}% on the test dataset')\n","\n","train_acc,CM_Train = test(model.cuda(), train_loader, criterion, True)\n","print(f'The model has achieved an accuracy of {100 * train_acc:.2f}% on the trian dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmAHB03ILAS8"},"outputs":[],"source":["best_Result = history[history[\"valid_acc\"]==history[\"valid_acc\"].max()]\n","Models_Results = {\n","      \"Model_Path\":[modelName],\n","      \"DataSetName\":[DataSetName],\n","\n","      \"train_acc_Best_Eb\":[best_Result.train_acc.values[0]],\n","      \"vali_acc_Best_Eb\":[best_Result.valid_acc.values[0]],\n","\n","      \"train_acc_All\":[train_acc],\n","      \"test_acc_All\":[test_acc],\n","      \"CM_Test\": [ str(np.array(CM_Test).reshape(7*7).tolist())],\n","      \"CM_Train\" :  [ str(np.array(CM_Train).reshape(7*7).tolist())]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mc86y_2st_l3"},"outputs":[],"source":["Save_To_CSV(PathToDF,pd.DataFrame.from_dict(Models_Results))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCOUIUcLrdSx"},"outputs":[],"source":["CM_Test.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRVsvvY3c81j"},"outputs":[],"source":["# target = torch.tensor([1, 1, 0, 0])\n","# preds = torch.tensor([0, 1, 0, 0])\n","# confmat = ConfusionMatrix(num_classes=2)\n","# o = confmat(preds, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h--EfAYm53r"},"outputs":[],"source":["# gih = pd.read_csv(\"test2.csv\",converters={'a': pd.eval})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3rf3QlHp1j6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"dbce3ab015948f9d7a38d0a4c6629e1339f6d90306f3d65fd7273a0ce3b29204"}}},"nbformat":4,"nbformat_minor":0}
